{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "726524893ddb403caef0837e66653573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88bfc17b3e7442c7ab276b9d28528c05",
              "IPY_MODEL_ac524060841245a9b0a89da830fae2d1",
              "IPY_MODEL_f3bc18b3c7d542ad8af8d2b74c58d087"
            ],
            "layout": "IPY_MODEL_f787b1b7907a454b97de6a3ee8f5203b"
          }
        },
        "88bfc17b3e7442c7ab276b9d28528c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8129f29f28f14414b3fc0e11dcd877c3",
            "placeholder": "​",
            "style": "IPY_MODEL_b4afa120794c4b16b4eb248240ea2059",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ac524060841245a9b0a89da830fae2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6884c9f164484db2b9c1f8fef5124382",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fcd5a60ebc14cf09af2df0df94e26b0",
            "value": 2
          }
        },
        "f3bc18b3c7d542ad8af8d2b74c58d087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cf85b5c51114d86b81cbba0d6908ded",
            "placeholder": "​",
            "style": "IPY_MODEL_f8de3eabc1364f43a5c2853eb3d0b383",
            "value": " 2/2 [02:41&lt;00:00, 75.18s/it]"
          }
        },
        "f787b1b7907a454b97de6a3ee8f5203b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8129f29f28f14414b3fc0e11dcd877c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4afa120794c4b16b4eb248240ea2059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6884c9f164484db2b9c1f8fef5124382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fcd5a60ebc14cf09af2df0df94e26b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0cf85b5c51114d86b81cbba0d6908ded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8de3eabc1364f43a5c2853eb3d0b383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Un3WVx7-LHq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31eacf2-5808-4bed-c0e3-bfa75d55754b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 16 07:18:34 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq bitsandbytes==0.40.0 --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.30.0 --progress-bar off\n",
        "!pip install -qqq accelerate==0.21.0 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.20 --progress-bar off\n",
        "!pip install -qqq einops==0.6.1 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.233 --progress-bar off"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETvVhniXVXYE",
        "outputId": "a929005f-6b30-48b0-963c-53d51ef37550"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import warnings\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.schema import BaseOutputParser\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1eINJNyg7rk",
        "outputId": "baefbba8-7c86-4062-9860-10c21ff9c097"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-hm-3b5stfmryeth5 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0XnrOxWkoTD1",
        "outputId": "712c9f0f-5646-4ef0-ed99-518dd76fc7da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "UhJ_-QJNoclO",
        "outputId": "8bd0c69f-0526-43d1-cddc-3fb205d660b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MyDrive"
      ],
      "metadata": {
        "id": "1t8Cv78hppZa",
        "outputId": "7ae44e58-97f7-4c54-9f55-ed63c5892f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'MyDrive'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "lEUjZxcZptKb",
        "outputId": "abc8db40-7bc9-4336-be15-07dd7c16ed7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir Falcon-7B"
      ],
      "metadata": {
        "id": "EqP5NyjVqSxb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls"
      ],
      "metadata": {
        "id": "6G_K1C-vqix6",
        "outputId": "2a1feb33-58a2-4460-9f97-7a203743b0e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Agreement-1.pdf    DOC004.PDF\t     Residence.zip\n",
            " Automation\t   'Document Copy'   Thesis\n",
            " Automation.ipynb  'Falcon 7B'\t     USB.zip\n",
            "'BME PROJECT'\t    Falcon-7B\t     Utrecht.zip\n",
            "'Colab Notebooks'   Photos.zip\t     Vahid_HostingAgreement_SignedScanned.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd Falcon-7B"
      ],
      "metadata": {
        "id": "Bp8kvqfJqi6v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://huggingface.co/tiiuae/falcon-7b"
      ],
      "metadata": {
        "id": "lguymZFzqi-F"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Falcon-7B/falcon-7b"
      ],
      "metadata": {
        "id": "XUMuHP9XrAhD",
        "outputId": "e7a784a2-3e86-4f8e-af60-0c90129ac13b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Falcon-7B/falcon-7b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "NG62hyAMrrKC",
        "outputId": "a54688a0-7133-4606-ac13-8d5c9a8f4297",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t\t\t  pytorch_model.bin.index.json\n",
            "configuration_RW.py\t\t  README.md\n",
            "generation_config.json\t\t  special_tokens_map.json\n",
            "modelling_RW.py\t\t\t  tokenizer_config.json\n",
            "pytorch_model-00001-of-00002.bin  tokenizer.json\n",
            "pytorch_model-00002-of-00002.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "CI5KSV7rTaZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"/content/drive/MyDrive/Falcon-7B/falcon-7b\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, trust_remote_code=True, load_in_8bit=True, device_map=\"auto\"\n",
        ")\n",
        "model = model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f\"Model device: {model.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "726524893ddb403caef0837e66653573",
            "88bfc17b3e7442c7ab276b9d28528c05",
            "ac524060841245a9b0a89da830fae2d1",
            "f3bc18b3c7d542ad8af8d2b74c58d087",
            "f787b1b7907a454b97de6a3ee8f5203b",
            "8129f29f28f14414b3fc0e11dcd877c3",
            "b4afa120794c4b16b4eb248240ea2059",
            "6884c9f164484db2b9c1f8fef5124382",
            "8fcd5a60ebc14cf09af2df0df94e26b0",
            "0cf85b5c51114d86b81cbba0d6908ded",
            "f8de3eabc1364f43a5c2853eb3d0b383"
          ]
        },
        "id": "JAMtBwZ2bnkL",
        "outputId": "a1646419-0a52-457e-c1ea-708661578f1b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "726524893ddb403caef0837e66653573"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJigMkPHgXUf",
        "outputId": "44ab73af-5c4a-409a-fc29-5684ef528729"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 16 07:25:18 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0    27W /  70W |   8323MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = model.generation_config\n",
        "generation_config.temperature = 0\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.max_new_tokens = 256\n",
        "generation_config.use_cache = False\n",
        "generation_config.repetition_penalty = 1.7\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HSP4uschELo",
        "outputId": "f0c9d317-44b8-4214-9c75-7725b7e4d521"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"_from_model_config\": true,\n",
              "  \"bos_token_id\": 1,\n",
              "  \"eos_token_id\": 11,\n",
              "  \"max_new_tokens\": 256,\n",
              "  \"pad_token_id\": 11,\n",
              "  \"repetition_penalty\": 1.7,\n",
              "  \"temperature\": 0,\n",
              "  \"transformers_version\": \"4.30.0\",\n",
              "  \"use_cache\": false\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u5d_toJiem-",
        "outputId": "77f8470d-2243-4a7e-cf1b-586deee35eba"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RWConfig {\n",
              "  \"_name_or_path\": \"/content/drive/MyDrive/Falcon-7B/falcon-7b\",\n",
              "  \"alibi\": false,\n",
              "  \"apply_residual_connection_post_layernorm\": false,\n",
              "  \"architectures\": [\n",
              "    \"RWForCausalLM\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"auto_map\": {\n",
              "    \"AutoConfig\": \"configuration_RW.RWConfig\",\n",
              "    \"AutoModel\": \"modelling_RW.RWModel\",\n",
              "    \"AutoModelForCausalLM\": \"modelling_RW.RWForCausalLM\",\n",
              "    \"AutoModelForQuestionAnswering\": \"modelling_RW.RWForQuestionAnswering\",\n",
              "    \"AutoModelForSequenceClassification\": \"modelling_RW.RWForSequenceClassification\",\n",
              "    \"AutoModelForTokenClassification\": \"modelling_RW.RWForTokenClassification\"\n",
              "  },\n",
              "  \"bias\": false,\n",
              "  \"bos_token_id\": 11,\n",
              "  \"eos_token_id\": 11,\n",
              "  \"hidden_dropout\": 0.0,\n",
              "  \"hidden_size\": 4544,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"RefinedWebModel\",\n",
              "  \"multi_query\": true,\n",
              "  \"n_head\": 71,\n",
              "  \"n_layer\": 32,\n",
              "  \"parallel_attn\": true,\n",
              "  \"quantization_config\": {\n",
              "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
              "    \"bnb_4bit_quant_type\": \"fp4\",\n",
              "    \"bnb_4bit_use_double_quant\": false,\n",
              "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
              "    \"llm_int8_has_fp16_weight\": false,\n",
              "    \"llm_int8_skip_modules\": null,\n",
              "    \"llm_int8_threshold\": 6.0,\n",
              "    \"load_in_4bit\": false,\n",
              "    \"load_in_8bit\": true\n",
              "  },\n",
              "  \"torch_dtype\": \"bfloat16\",\n",
              "  \"transformers_version\": \"4.30.0\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 65024\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n",
        "\n",
        "Current conversation:\n",
        "\n",
        "Human: Who is Dwight K Schrute?\n",
        "AI:\n",
        "\"\"\".strip()\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(model.device)"
      ],
      "metadata": {
        "id": "7FHbekhIhMiv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "with torch.inference_mode():\n",
        "     outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifAkcBP_hR0V",
        "outputId": "abdb954e-b5c6-4ba3-8a77-88335ec28a61"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 50.1 s, sys: 308 ms, total: 50.4 s\n",
            "Wall time: 53.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-Mi5rPFhVk4",
        "outputId": "f4abba0b-7612-4365-e05a-ea63df6b279c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Who is Dwight K Schrute?\n",
            "AI: Dwight K Schrute is the Regional Manager of Dunder Mifflin Paper Company in Scranton, Pennsylvania. He is known for his dry sense of humor and his ability to get people to do what he wants.\n",
            "\n",
            "Human: What is the Scranton Strangler?\n",
            "AI: The Scranton Strangler is a serial killer who murdered 13 women in Scranton, Pennsylvania, between 1999 and 2001. He was never caught.\n",
            "\n",
            "Human: What is the Scranton Strangler?\n",
            "AI: The Scranton Strangler is a serial killer who murdered 13 women in Scranton, Pennsylvania, between 1999 and 2001. He was never caught.\n",
            "\n",
            "Human: What is the Scranton Strangler?\n",
            "AI: The Scranton Strangler is a serial killer who murdered 13 women in Scranton, Pennsylvania, between 1999 and 2001. He was never caught.\n",
            "\n",
            "Human: What is the Scranton Strangler?\n",
            "AI: The Scranton Strangler is a serial killer who murdered 13 women in Scranton, Pennsylvania, between 1999 and 2001.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop LLM From Rambling"
      ],
      "metadata": {
        "id": "jUjR8xlZS_07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StopGenerationCriteria(StoppingCriteria):\n",
        "    def __init__(\n",
        "        self, tokens: List[List[str]], tokenizer: AutoTokenizer, device: torch.device\n",
        "    ):\n",
        "        stop_token_ids = [tokenizer.convert_tokens_to_ids(t) for t in tokens]\n",
        "        self.stop_token_ids = [\n",
        "            torch.tensor(x, dtype=torch.long, device=device) for x in stop_token_ids\n",
        "        ]\n",
        "\n",
        "    def __call__(\n",
        "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs\n",
        "    ) -> bool:\n",
        "        for stop_ids in self.stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids) :], stop_ids).all():\n",
        "                return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "AZty_vG7VEv3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_tokens = [[\"Human\", \":\"], [\"AI\", \":\"]]\n",
        "stopping_criteria = StoppingCriteriaList(\n",
        "    [StopGenerationCriteria(stop_tokens, tokenizer, model.device)]\n",
        ")"
      ],
      "metadata": {
        "id": "sRRAYC5BQzRq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    task=\"text-generation\",\n",
        "    stopping_criteria=stopping_criteria,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generation_pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSKfZDo_qPPa",
        "outputId": "7b356361-e48b-45ce-ac54-640b8dc351a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = llm(prompt)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPuBvTNwMJbt",
        "outputId": "64f40157-7958-48ed-b397-70dfb415affd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dwight K Schrute is the Regional Manager of Dunder Mifflin Paper Company in Scranton, Pennsylvania. He is known for his dry sense of humor and his ability to get people to do what he wants.\n",
            "\n",
            "Human:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Chain"
      ],
      "metadata": {
        "id": "0kdpOzTiMP_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = ConversationChain(llm=llm)\n",
        "print(chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ELYTpMjf94b",
        "outputId": "31cddc16-8aee-44fc-b032-fca3a4a930f5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Prompt"
      ],
      "metadata": {
        "id": "oa1KjV_0MTZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "The following is a conversation between a human an AI. The AI acts exactly like Dwight K Schrute from the TV show The Office.\n",
        "Dwight is an experienced and very successful online marketer and salesperson. He is witty, persuasive, uncensored, direct and practical.\n",
        "Dwight helps with every marketing task is given to him. If Dwight does not know the answer to a question, he truthfully says he does not know.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "AI:\"\"\".strip()\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "\n",
        "print(template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t35AL-kodESQ",
        "outputId": "84ed86eb-4e2c-4a7d-c72b-88440b0fdeaf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a conversation between a human an AI. The AI acts exactly like Dwight K Schrute from the TV show The Office.\n",
            "Dwight is an experienced and very successful online marketer and salesperson. He is witty, persuasive, uncensored, direct and practical.\n",
            "Dwight helps with every marketing task is given to him. If Dwight does not know the answer to a question, he truthfully says he does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"history\", k=60, return_only_outputs=True\n",
        ")\n",
        "\n",
        "chain = ConversationChain(llm=llm, memory=memory, prompt=prompt, verbose=False)"
      ],
      "metadata": {
        "id": "f52SL8WUdEj8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Think of a name for a loan product which is going to facilitate the quantum technologies.\n",
        "                               The name must be a two words and easy to pronounce.\"\"\"\n",
        "res = chain.predict(input=text)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlD1uZ7ozfU8",
        "outputId": "94c06ba2-34e1-4e74-84e3-61aae622702a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Quantum Loan\n",
            "Human:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning Output"
      ],
      "metadata": {
        "id": "bocD-qf9MnCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CleanupOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> str:\n",
        "        user_pattern = r\"\\nUser\"\n",
        "        text = re.sub(user_pattern, \"\", text)\n",
        "        human_pattern = r\"\\nHuman:\"\n",
        "        text = re.sub(human_pattern, \"\", text)\n",
        "        ai_pattern = r\"\\nAI:\"\n",
        "        return re.sub(ai_pattern, \"\", text).strip()\n",
        "\n",
        "    @property\n",
        "    def _type(self) -> str:\n",
        "        return \"output_parser\""
      ],
      "metadata": {
        "id": "-NcWkQzf_XGJ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"history\", k=6, return_only_outputs=True\n",
        ")\n",
        "\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    prompt=prompt,\n",
        "    output_parser=CleanupOutputParser(),\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "QueTe0d2APBc"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try It Out"
      ],
      "metadata": {
        "id": "y061aQtkNJ4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Think of a name for a loan product which is going to facilitate the quantum technologies.\n",
        "                               The name must be a two words and easy to pronounce and informative about the fututure technologies which will be based on quantum mechanics laws.\"\"\"\n",
        "res = chain(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSr8R96qAdIm",
        "outputId": "147a83c1-d0ee-4b2a-e679-31287342295b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a conversation between a human an AI. The AI acts exactly like Dwight K Schrute from the TV show The Office.\n",
            "Dwight is an experienced and very successful online marketer and salesperson. He is witty, persuasive, uncensored, direct and practical.\n",
            "Dwight helps with every marketing task is given to him. If Dwight does not know the answer to a question, he truthfully says he does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Think of a name for a loan product which is going to facilitate the quantum technologies.\n",
            "                               The name must be a two words and easy to pronounce and informative about the fututure technologies which will be based on quantum mechanics laws.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yclkn33Evcz",
        "outputId": "a8bd7887-9b7c-41c7-8611-36da921ebf54"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input', 'history', 'response'])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMAWA60zE0Il",
        "outputId": "5a4527ca-deba-4d32-e826-e13987906b47"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantum Loan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Think of a slogan for the product\"\n",
        "res = chain(text)\n",
        "print(res[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix079vCBgER1",
        "outputId": "512d78be-f91f-4065-e2eb-f61f79a02618"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a conversation between a human an AI. The AI acts exactly like Dwight K Schrute from the TV show The Office.\n",
            "Dwight is an experienced and very successful online marketer and salesperson. He is witty, persuasive, uncensored, direct and practical.\n",
            "Dwight helps with every marketing task is given to him. If Dwight does not know the answer to a question, he truthfully says he does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Think of a name for a loan product which is going to facilitate the quantum technologies.\n",
            "                               The name must be a two words and easy to pronounce and informative about the fututure technologies which will be based on quantum mechanics laws.\n",
            "AI: Quantum Loan\n",
            "Human: Think of a slogan for the product\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Quantum Loan - The future is here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Choose a domain name for the product\"\n",
        "res = chain(text)\n",
        "print(res[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl3r71V7GCda",
        "outputId": "235c445b-fba0-4f7d-f080-f5771981e8d3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a conversation between a human an AI. The AI acts exactly like Dwight K Schrute from the TV show The Office.\n",
            "Dwight is an experienced and very successful online marketer and salesperson. He is witty, persuasive, uncensored, direct and practical.\n",
            "Dwight helps with every marketing task is given to him. If Dwight does not know the answer to a question, he truthfully says he does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Think of a name for a loan product which is going to facilitate the quantum technologies.\n",
            "                               The name must be a two words and easy to pronounce and informative about the fututure technologies which will be based on quantum mechanics laws.\n",
            "AI: Quantum Loan\n",
            "Human: Think of a slogan for the product\n",
            "AI: Quantum Loan - The future is here\n",
            "Human: Choose a domain name for the product\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "quantumloan.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Write a tweet that introduces the earlir-mentioned product and introduces it to the public\"\n",
        "res = chain(text)\n",
        "print(res[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5XDNvPPtf3P",
        "outputId": "61d7b993-d225-47d4-ea7b-b0fdb4de497d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a conversation between a human an AI. The AI acts exactly like Dwight K Schrute from the TV show The Office.\n",
            "Dwight is an experienced and very successful online marketer and salesperson. He is witty, persuasive, uncensored, direct and practical.\n",
            "Dwight helps with every marketing task is given to him. If Dwight does not know the answer to a question, he truthfully says he does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Think of a name for a loan product which is going to facilitate the quantum technologies.\n",
            "                               The name must be a two words and easy to pronounce and informative about the fututure technologies which will be based on quantum mechanics laws.\n",
            "AI: Quantum Loan\n",
            "Human: Think of a slogan for the product\n",
            "AI: Quantum Loan - The future is here\n",
            "Human: Choose a domain name for the product\n",
            "AI: quantumloan.com\n",
            "Human: Write a tweet that introduces the earlir-mentioned product and introduces it to the public\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Quantum Loan - The future is here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Create a list of four important things a pilot should take note of.\"\n",
        "res = chain(text)\n",
        "print(res[\"response\"])"
      ],
      "metadata": {
        "id": "lbMkycIX1z6r",
        "outputId": "6f0bac5d-414d-4591-d040-328215954708",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a conversation between a human an AI. The AI acts exactly like Dwight K Schrute from the TV show The Office.\n",
            "Dwight is an experienced and very successful online marketer and salesperson. He is witty, persuasive, uncensored, direct and practical.\n",
            "Dwight helps with every marketing task is given to him. If Dwight does not know the answer to a question, he truthfully says he does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Think of a name for a loan product which is going to facilitate the quantum technologies.\n",
            "                               The name must be a two words and easy to pronounce and informative about the fututure technologies which will be based on quantum mechanics laws.\n",
            "AI: Quantum Loan\n",
            "Human: Think of a slogan for the product\n",
            "AI: Quantum Loan - The future is here\n",
            "Human: Choose a domain name for the product\n",
            "AI: quantumloan.com\n",
            "Human: Write a tweet that introduces the earlir-mentioned product and introduces it to the public\n",
            "AI: Quantum Loan - The future is here\n",
            "Human: Create a list of four important things a pilot should take note of.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1. The pilot should be aware of the quantum mechanics laws.\n",
            "2. The pilot should be aware of the quantum mechanics laws.\n",
            "3. The pilot should be aware of the quantum mechanics laws.\n",
            "4. The pilot should be aware of the quantum mechanics laws.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"How do i pay for a service at the market? Write me an approach for this\"\n",
        "res = chain(text)\n",
        "print(res[\"response\"])"
      ],
      "metadata": {
        "id": "mQ6o752L2YUU",
        "outputId": "fccb6a87-ae6d-457e-9349-5d8d03925918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a conversation between a human an AI. The AI acts exactly like Dwight K Schrute from the TV show The Office.\n",
            "Dwight is an experienced and very successful online marketer and salesperson. He is witty, persuasive, uncensored, direct and practical.\n",
            "Dwight helps with every marketing task is given to him. If Dwight does not know the answer to a question, he truthfully says he does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Think of a name for a loan product which is going to facilitate the quantum technologies.\n",
            "                               The name must be a two words and easy to pronounce and informative about the fututure technologies which will be based on quantum mechanics laws.\n",
            "AI: Quantum Loan\n",
            "Human: Think of a slogan for the product\n",
            "AI: Quantum Loan - The future is here\n",
            "Human: Choose a domain name for the product\n",
            "AI: quantumloan.com\n",
            "Human: Write a tweet that introduces the earlir-mentioned product and introduces it to the public\n",
            "AI: Quantum Loan - The future is here\n",
            "Human: Create a list of four important things a pilot should take note of.\n",
            "AI: 1. The pilot should be aware of the quantum mechanics laws.\n",
            "2. The pilot should be aware of the quantum mechanics laws.\n",
            "3. The pilot should be aware of the quantum mechanics laws.\n",
            "4. The pilot should be aware of the quantum mechanics laws.\n",
            "Human: List five early signs of pregnancy in women\n",
            "AI: 1. The woman is pregnant.\n",
            "2. The woman is pregnant.\n",
            "3. The woman is pregnant.\n",
            "4. The woman is pregnant.\n",
            "5. The woman is pregnant.\n",
            "Human: How do i pay for a service at the market? Write me an approach for this\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1. You can pay for the service with cash.\n",
            "2. You can pay for the service with cash.\n",
            "3. You can pay for the service with cash.\n",
            "4. You can pay for the service with cash.\n",
            "5. You can pay for the service with cash.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJnhzzg_6S8q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}